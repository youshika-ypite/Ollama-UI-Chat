# Интерфейс на PySide6 для взаимодействия с локальными LLM моделями Ollama

Представляет из себя диалоговое окно с возможностью написать текст и отправить его Ollama. После чего получить ответ.
* По умолчанию введена llama3.1
* История переписки сохраняется, можно продолжить ее в любой момент

Данная версия *тестовая* и будет развиваться в менеджер чатов. На данный момент я больше эксперементирую, поэтому развитие очень медленное.

Планируются возможности:
* Создавать отдельные динамические чаты с разными моделями и надстройками.
* Учет и анализ чатов, закрепление.

## Установка
```bash
git clone https://github.com/youshika-ypite/Ollama-chat-with-UI.git

python -m venv venv
venv\Scripts\activate

pip install Ollama, PySide6
```
## Запуск
```bash
venv\Scripts\activate
chat.py
```

## Модель
Изменить ollama модель можно в файле конфигурации `data/config.json`

## Интерфейс
Рядом с кнопкой отправки сообщения предлагается изменить тему интерфейса на темную/светлую. По умолчанию используется светлая.