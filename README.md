# Интерфейс на PySide6 для взаимодействия с локальными LLM моделями Ollama

Представляет из себя диалоговое окно с возможностью написать текст и отправить его Ollama. После чего получить ответ.
* По умолчанию введена llama3.1.
* История переписки сохраняется, можно продолжить ее в любой момент.
* Возможность создавать разные чаты для одной модели (в буд. будет изменено).

Данная версия *тестовая* и будет развиваться в менеджер чатов. На данный момент я больше экспериментирую, поэтому развитие очень медленное.

Планируются возможности:
* Создавать отдельные динамические чаты с разными моделями и надстройками.
* Учет и анализ чатов, закрепление.

## Установка
```bash
git clone https://github.com/youshika-ypite/Ollama-chat-with-UI.git

python -m venv venv
venv\Scripts\activate

pip install Ollama, PySide6
```
## Запуск
```bash
venv\Scripts\activate
chat.py
```

## Модель
Изменить ollama модель можно в файле конфигурации `data/config.json`

## Интерфейс
В статус баре (правый нижний угол окна) сообщения предлагается изменить тему интерфейса на темную/светлую. По умолчанию используется светлая.
Также в левом нижнем располагается кнопка создания нового чата.
Кнопка удаления чата подписана как !DEL